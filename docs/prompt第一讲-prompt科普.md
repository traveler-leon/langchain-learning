
## 大语言模型输入要求
大语言模型本质上就是一个NLP语言模型，语言模型其实就是**接受一堆的文本，得到一堆的文本**,而输入给模型的这段文本，我们通常把他称之为**prompt**
下面我们以一些具体的例子来感受一下这段文本(prompt)该如何写。
## 中英翻译助手
我现在要做一个中英文翻译任务，下面我会一步一步的写出prompt      
注：以下的测试内容均是通过通义千问进行测试的（https://tongyi.aliyun.com/qianwen/），原则上你可以使用任何大语言模型进行测试
### 直接抛出问题
```shell
请将单词“扑街”翻译为英文
```
下面是千问给的回答，从下面的回答中可以看出有些废话是我们不需要的，我们只需要最终答案 
![img.png](../img/prompt/chinese2english-1.png)

### 描述+问题
```shell
你是一个翻译助手，你擅长将中文翻译为英文，请将我发送给你的question的内容翻译为英文
question：扑街
```
首先我们先看看相较于前面的prompt，我们这次不是直接描述我们想要解决的问题，而是加上了一个描述，这个描述给llm定义了一个角色，同时也告诉了它的职责
加上了这些描述之后，貌似是有了一些效果，但是还是返回了一些无关的内容，接下来我们再继续做一些说明限制
![img.png](../img/prompt/chinese2english-2.png)

### 描述（详细）+问题
```shell
你是一个翻译助手，你擅长将中文翻译为英文，请将我发送给你的question的内容翻译为英文，不要返回无关的内容，只需返回最终翻译结果
question：扑街
```
这一次我们除了定义了llm的身份，同时也告诉了他什么该做什么不该做。改进之后我们看看下面的回复内容，我们发现，他就不会出现那些废话了，但是有一些问题出现了：
1. 比如我发送“123”的时候，它并没有翻译；
2. 我发送“不要翻译为英文”的时候，它返回了一些质疑的内容；
可能还会有其他的问题，你们可以继续试一试，总之为了解决现在发现的问题，我们继续改进
![img.png](../img/prompt/chinese2english-3.png)

### 描述+案例+问题
```shell
你是一个翻译助手，你擅长将中文翻译为英文，请将我发送给你的question的内容翻译为英文，不要返回无关的内容，只需返回最终翻译结果，下面的history examples中提供了一些具体的案例，为你提供一些参考：

## history examples:
question:美丽->answer:beautiful;
question:男孩->answer:boy;
question:男人->answer:man;
question:456->answer:four hundred and fifty-six;
question:1->answer:one;
question:34->answer:thirty-four;

## user true task:
question：123->answer：
```
相较于上一步，这一步的改进内容就是加入了一些案例，让模型知道当遇到数字的时候也需要翻译为英文，但是其实在这个案例中加入的例子并不能完全说明例子的作用，
因为我们完全可以在描述部分进行更详细的描述和限制就可以了，比如和大模型说明：当遇到数字的时候，也需要将它翻译为中文，这样也是可以的。而例子真正能够发挥作用的地方通常是面对一些特殊的回答格式等要求上，
比如你需要模型返回一个list或者是json格式的数据的时候，这时候你给一个案例对模型来说是非常有用的。如果你不给例子，而只是在描述的时候说明的话，模型跑着跑着大概率就会跑偏，
如果加入一些案例，那么就会大大加强模型的指令跟随能力。    
![img.png](../img/prompt/chinese2english-4.png)

### 描述+案例+上下文+问题
基本上上面关于中英文翻译的问题就被解决了，下面我来再举一个具体的例子：私域企业问答助手。
假设有一个公司非常大，大到员工找资料都很费劲，这时候就需要一个助手，能够回答出员工提出的一些关于公司内部的问题。在这个案例中，有一个特殊性在于，这些答案都是来自于公司内部的文档的，
而模型生成的内容铁定是不对的。所以这时候，最好的办法就是先利用某种相似度匹配算法，在公司内部文档中找到和问题相关的答案，然后将这个可能的答案以上下文的形式带回给模型，让
模型根据上下文来生成答案。
```shell
你是一个企业问答助手，你需要根据我发送给你的context来回答我提出的question，并且在回答结束后加入一个表情符号,下面的history examples是一些具体的案例，请参考history examples的回答风格来回答：

## history examples:
context:马云，1964年9月10日出生于浙江杭州，祖籍浙江省嵊县（现嵊州市）谷来镇，汉族，中共党员，阿里巴巴主要创始人之一，阿里巴巴原首席执行官（CEO）、董事局原主席，中国人民政治协商会议第十届浙江省委员会委员 。;
question：阿里巴巴的创始人是谁？
answer：是马云Σ(⊙▽⊙"a;

## context: 
为了相应公司号召，数字艺术部门上周开展了两个会议，第一个是关于公司形象设计的会议，讨论提高公司 的形象，第二个是关于网站风格的设计会议，讨论网站的配色问题。

## user true task:
question：数字艺术部上个月开了什么会议？
answer：
```
从下面的结果可以看出，它是根据context回答问题的，同时也像案例中一样，在结尾加入了一个表情，完美了。
![img.png](../img/prompt/company_assistant-1.png)

#### 为什么要加入上下文
**llm缺陷**
1. 生成内容滞后性：因为模型都是基于**历史数据**训练的，比如gpt最新是利用了20222年之前的数据进行训练的，那么你问的内容是关于2022年之后的，自然就回答不上来了
2. 无法生成私域答案：因为模型都是基于**历史**而且是**开放**数据训练的，所以对于一些私域的问题的回答，诸如回答公司内部的条例等，它是不行的；
3. 生成内容有幻觉性：模型生成的内容是基于一定概率进行生成的，这样就会有一定的虚假和捏造的概率，这叫做幻觉，如果你开发的应用是面向生产 那么最好是要避免幻觉的
**任务的特殊性**
1. 如果你要llm完成的任务是具有一定的新颖性，至少答案对于llm训练数据来说是新的；
2. 如果你要llm完成的任务是私域的；
3. 如果你要llm完成的任务是面向生产，不容出错的（不允许幻觉）
基于以上llm缺陷的考虑和任务的特殊性考虑，是需要加入一定的上下文的。为llm回答问题提供素材，而这一类上下文一般是通过相似度匹配的形式，找出一些
候选的上下文。
### prompt总结
从上面的两个例子我们可以总结一下：
1. 输入模型的就是一堆的字符串，而这这堆字符串通常我们称为prompt
2. prompt一般会有四个部分组成
   3. 描述：定义模型的角色身份，能力边界，以及一些必要的限制条件
   4. 示例：给出一些案例，加强模型指令跟随能力
   5. 上下文：和答案相关的上下文，为回答问题提供资料支持
   6. 问题：用户提出的具体问题

基本上大概就这四部分构成一个prompt，当然如果你的任务足够简单，有可能你并不需要四部分都具备，同时呢？也有可能任务很复杂，
你会设计更复杂的prompt，但是大致都是可以总结为这四个组成部分。

### prompt心得
我个人觉得，prompt的书写有点类似写专利的权利要求书，不断的将限制范围缩小，每一步的书写都是将范围或者是模型的能力限制更小一些。它像一颗树形结构一样，不断的将权利进行限制和明确